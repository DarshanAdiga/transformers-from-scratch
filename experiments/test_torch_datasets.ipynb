{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example notebook to download and use PyTorch text datasets\n",
    "In this notebook, [YelpReviewPolarity](https://pytorch.org/text/stable/datasets.html#torchtext.datasets.YelpReviewPolarity) is used.\n",
    "\n",
    "**References:**\n",
    "- https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext.datasets import YelpReviewPolarity\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './data_tmp'\n",
    "if not os.path.isdir(DATA_DIR):\n",
    "    os.mkdir(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "t_device\n",
    "#TODO Use this device in all the places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torchtext.data.datasets_utils._RawTextIterableDataset at 0x7f2ecdf56c70>,\n",
       " <torchtext.data.datasets_utils._RawTextIterableDataset at 0x7f2ecdecfd90>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WARN: If  you get an error regarding confirm_token, restart the notebook and run again\n",
    "train_dataset_iter, test_dataset_iter = YelpReviewPolarity(DATA_DIR)\n",
    "(train_dataset_iter, test_dataset_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Configurations\n",
    "Get the dataset configurations necessary for building the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of Out-of-vocabulary words (<unk>) is:0\n"
     ]
    }
   ],
   "source": [
    "# First, create a tokenizer and vocabulary for the given dataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizer = get_tokenizer(\"subword\")\n",
    "\n",
    "def yield_tokenizer(data_iterator):\n",
    "    # We don't need the label_id here\n",
    "    for label_id, text in data_iterator:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "# Create a vocabulary from the training corpus\n",
    "# Note: these iterators are one-off and cannot be reused \n",
    "train_dataset_iter = YelpReviewPolarity(DATA_DIR, split='train')\n",
    "MIN_FREQ=2 # Include only those tokens with a frequency of MIN_FREQ or greater in the vocabulary \n",
    "vocabulary = build_vocab_from_iterator(yield_tokenizer(train_dataset_iter), min_freq=MIN_FREQ, specials=[\"<unk>\"])\n",
    "vocabulary.set_default_index(vocabulary[\"<unk>\"])\n",
    "oov_index = vocabulary.get_default_index()\n",
    "print(f\"Index of Out-of-vocabulary words (<unk>) is:{oov_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In practice, we define various data processing steps as a pipeline\n",
    "# Convert text to sequence of numbers\n",
    "text_pipeline = lambda text: vocabulary(tokenizer(text))\n",
    "# Labels are [1, 2], convert them into [0, 1]\n",
    "label_pipeline = lambda label: int(label) - 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 128\n",
    "PADDING_INDEX = 1 #TODO Is this right thing to do?\n",
    "\n",
    "def preprocess_data_using_collate_batch(batch):\n",
    "    \"\"\"Given a batch of samples generated by DataLoader. This collate function uses the data processing \\\n",
    "        pipelines defined earlier.\n",
    "\n",
    "        Both padding and truncation (upto MAX_SEQ_LEN) are applied within every batch.\n",
    "        Reference: https://pytorch.org/text/main/tutorials/sst2_classification_non_distributed.html\n",
    "\n",
    "    Args:\n",
    "        batch (iterator): A batch of (label, text) samples\n",
    "\n",
    "    Returns:\n",
    "        tuple: (batch_label_tensor, batch_token_id_tensor)\n",
    "    \"\"\"\n",
    "    label_list, text_tensor_list = [], []\n",
    "\n",
    "    # Process and accumulate the samples within this batch\n",
    "    for label, text in batch:\n",
    "        # Preprocess & Accumulate the labels\n",
    "        label = label_pipeline(label)\n",
    "        label_list.append(label)\n",
    "\n",
    "        #  Preprocess & Accumulate the texts\n",
    "        text_token_ids = text_pipeline(text)\n",
    "        # TODO Find if pytorchtext (0.11 version) supports truncation & padding in any way\n",
    "        if len(text_token_ids) > MAX_SEQ_LEN:\n",
    "            # Truncation\n",
    "            text_token_ids = text_token_ids[:MAX_SEQ_LEN]\n",
    "        else:\n",
    "            # Padding\n",
    "            len_to_be_padded = MAX_SEQ_LEN - len(text_token_ids)\n",
    "            pad = [PADDING_INDEX] * len_to_be_padded\n",
    "            text_token_ids = text_token_ids + pad\n",
    "        \n",
    "        # Create the tensor from the token-id sequence\n",
    "        text_tensor = torch.tensor(text_token_ids, dtype=torch.int64)\n",
    "        text_tensor_list.append(text_tensor) # A list of tensors\n",
    "    \n",
    "    # Aggregate the sample tensors into bulk tensors; \n",
    "    \n",
    "    # Bulk tensor for label\n",
    "    batch_label_tensor = torch.tensor(label_list, dtype=torch.int64)\n",
    "\n",
    "    # Bulk tensor for text; convert a list of tensor objects into a single tensor object\n",
    "    batch_token_id_tensor = torch.stack(text_tensor_list)\n",
    "\n",
    "    return batch_label_tensor, batch_token_id_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # [Only for Test] Create the one-off iterators and then the dataloaders\n",
    "# from torch.utils.data import DataLoader\n",
    "# _, test_dataset_iter = YelpReviewPolarity(DATA_DIR)\n",
    "# test_dataloader = DataLoader(test_dataset_iter, batch_size=8, shuffle=False, collate_fn=preprocess_data_using_collate_batch)\n",
    "# for label,text in test_dataloader:\n",
    "#     print(label.size())\n",
    "#     print(text.size())\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH_SIZE EPOCHS EMB_SIZE VOCAB_SIZE MAX_SEQ_LEN CLASSES\n",
      "8 \t 10 \t 128 \t 153157 \t 128 \t {1, 2}\n"
     ]
    }
   ],
   "source": [
    "# The configurations\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 8\n",
    "EMB_SIZE = 128\n",
    "VOCAB_SIZE = len(vocabulary)\n",
    "# Note: these iterators are one-off and cannot be reused \n",
    "train_dataset_iter = YelpReviewPolarity(DATA_DIR, split='train')\n",
    "CLASSES = set([label for label,_ in train_dataset_iter])\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "\n",
    "print('BATCH_SIZE', 'EPOCHS', 'EMB_SIZE', 'VOCAB_SIZE', 'MAX_SEQ_LEN', 'CLASSES')\n",
    "print(BATCH_SIZE, '\\t', EPOCHS, '\\t', EMB_SIZE, '\\t', VOCAB_SIZE, '\\t', MAX_SEQ_LEN, '\\t', CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions to train and test the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "def train_one_epoch(model, train_dataloader, optimizer, criterion, clip_norm, cur_epoch, log_interval=500):\n",
    "    \"\"\"Train the 'model' on one epoch on the 'train_dataloader'.\n",
    "    Returns the model object back.\n",
    "    \"\"\"\n",
    "    model.train() # Train mode\n",
    "    train_size = len(train_dataloader)\n",
    "    cur_true_count, cur_sample_count = 0,0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for iteration, (batch_labels, batch_token_seq) in enumerate(train_dataloader):\n",
    "        # Reset grads\n",
    "        optimizer.zero_grad()\n",
    "        # Predict\n",
    "        batch_prediction = model(batch_token_seq)\n",
    "        # Compute loss\n",
    "        loss = criterion(batch_prediction, batch_labels)\n",
    "        # Compute the gradients\n",
    "        loss.backward()\n",
    "        # Clip the gradients; to prevent exploding gradients\n",
    "        clip_grad_norm_(model.parameters(), max_norm=clip_norm)\n",
    "        # Update the network parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute the accuracy metrics\n",
    "        pred_labels = batch_prediction.argmax(dim=1)\n",
    "        true_count = (pred_labels == batch_labels).sum().item()\n",
    "        cur_true_count += true_count\n",
    "        cur_sample_count += batch_labels.size(dim=0) # Same as doing .size()[0]\n",
    "        cur_accuracy = cur_true_count/cur_sample_count\n",
    "\n",
    "        # Log the metrics\n",
    "        if iteration % log_interval == 0 and iteration > 0:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(f\"Epoch: {cur_epoch:3d} \\t Batches: {iteration:5d}/{train_size:5d} \\t \\\n",
    "                Time: {elapsed_time:5.2f}s \\t Train Accuracy: {cur_accuracy:8.3f}\")\n",
    "\n",
    "            # reset the metrics\n",
    "            cur_true_count, cur_sample_count = 0,0\n",
    "            start_time = time.time()\n",
    "\n",
    "    # Return the model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, eval_dataloader):\n",
    "    \"\"\"Evaluate the model on eval_dataloader and return the accuracy\n",
    "    \"\"\"\n",
    "    final_true_count, final_sample_count = 0,0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (batch_labels, batch_token_seq) in eval_dataloader:\n",
    "            batch_prediction = model(batch_token_seq)\n",
    "            pred_labels = batch_prediction.argmax(dim=1)\n",
    "            true_count = (pred_labels == batch_labels).sum().item()\n",
    "\n",
    "            final_true_count += true_count\n",
    "            final_sample_count += batch_labels.size(dim=0)\n",
    "\n",
    "        # Compute & return the accuracy\n",
    "        return (final_true_count/final_sample_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare train & validation splits**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "# Create the one-off iterators and then the dataloaders\n",
    "train_dataset_iter, test_dataset_iter = YelpReviewPolarity(DATA_DIR)\n",
    "# Convert iterable-type datasets into map-type datasets\n",
    "train_dataset = to_map_style_dataset(train_dataset_iter)\n",
    "test_dataset = to_map_style_dataset(test_dataset_iter)\n",
    "\n",
    "# Split 95% and 5% for train & validation sets\n",
    "total_train_size = len(train_dataset)\n",
    "train_size = int(total_train_size * 0.95)\n",
    "train_split, val_split = random_split(train_dataset, [train_size, total_train_size-train_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataloaders for train and test datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create the dataloaders\n",
    "train_dataloader = DataLoader(train_split, batch_size=BATCH_SIZE, shuffle=True, collate_fn=preprocess_data_using_collate_batch)\n",
    "val_dataloader = DataLoader(val_split, batch_size=BATCH_SIZE, shuffle=False, collate_fn=preprocess_data_using_collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=preprocess_data_using_collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Test\n",
    "**Train the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerClassifier(\n",
       "  (token_embedding_layer): Embedding(153157, 128)\n",
       "  (position_embedding_layer): Embedding(128, 128)\n",
       "  (transformer_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (multihead_attention): MultiHeadSelfAttention(\n",
       "        (layer_weight_keys): Linear(in_features=128, out_features=1024, bias=False)\n",
       "        (layer_weight_queries): Linear(in_features=128, out_features=1024, bias=False)\n",
       "        (layer_weight_values): Linear(in_features=128, out_features=1024, bias=False)\n",
       "        (layer_merge_attention_heads): Linear(in_features=1024, out_features=128, bias=True)\n",
       "      )\n",
       "      (norm_layer1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_forward): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "      (norm_layer2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (multihead_attention): MultiHeadSelfAttention(\n",
       "        (layer_weight_keys): Linear(in_features=128, out_features=1024, bias=False)\n",
       "        (layer_weight_queries): Linear(in_features=128, out_features=1024, bias=False)\n",
       "        (layer_weight_values): Linear(in_features=128, out_features=1024, bias=False)\n",
       "        (layer_merge_attention_heads): Linear(in_features=1024, out_features=128, bias=True)\n",
       "      )\n",
       "      (norm_layer1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_forward): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "      (norm_layer2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (multihead_attention): MultiHeadSelfAttention(\n",
       "        (layer_weight_keys): Linear(in_features=128, out_features=1024, bias=False)\n",
       "        (layer_weight_queries): Linear(in_features=128, out_features=1024, bias=False)\n",
       "        (layer_weight_values): Linear(in_features=128, out_features=1024, bias=False)\n",
       "        (layer_merge_attention_heads): Linear(in_features=1024, out_features=128, bias=True)\n",
       "      )\n",
       "      (norm_layer1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_forward): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "      (norm_layer2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (multihead_attention): MultiHeadSelfAttention(\n",
       "        (layer_weight_keys): Linear(in_features=128, out_features=1024, bias=False)\n",
       "        (layer_weight_queries): Linear(in_features=128, out_features=1024, bias=False)\n",
       "        (layer_weight_values): Linear(in_features=128, out_features=1024, bias=False)\n",
       "        (layer_merge_attention_heads): Linear(in_features=1024, out_features=128, bias=True)\n",
       "      )\n",
       "      (norm_layer1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_forward): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "      (norm_layer2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (multihead_attention): MultiHeadSelfAttention(\n",
       "        (layer_weight_keys): Linear(in_features=128, out_features=1024, bias=False)\n",
       "        (layer_weight_queries): Linear(in_features=128, out_features=1024, bias=False)\n",
       "        (layer_weight_values): Linear(in_features=128, out_features=1024, bias=False)\n",
       "        (layer_merge_attention_heads): Linear(in_features=1024, out_features=128, bias=True)\n",
       "      )\n",
       "      (norm_layer1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_forward): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "      (norm_layer2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (multihead_attention): MultiHeadSelfAttention(\n",
       "        (layer_weight_keys): Linear(in_features=128, out_features=1024, bias=False)\n",
       "        (layer_weight_queries): Linear(in_features=128, out_features=1024, bias=False)\n",
       "        (layer_weight_values): Linear(in_features=128, out_features=1024, bias=False)\n",
       "        (layer_merge_attention_heads): Linear(in_features=1024, out_features=128, bias=True)\n",
       "      )\n",
       "      (norm_layer1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_forward): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "      (norm_layer2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (output_linear_layer): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the model class\n",
    "import sys\n",
    "sys.path.insert(0, '../mini-self-attention/')\n",
    "from transformer_classifier import TransformerClassifier\n",
    "\n",
    "classifier_model = TransformerClassifier(emb_size=EMB_SIZE, heads=8, num_of_blocks=6, seq_len=MAX_SEQ_LEN, \\\n",
    "    vocab_size=VOCAB_SIZE, num_classes=NUM_CLASSES)\n",
    "\n",
    "classifier_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE=0.1\n",
    "lr_warmup_after_epochs=5\n",
    "gradient_clip = 1.0\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(classifier_model.parameters(), lr=LEARNING_RATE)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=lr_warmup_after_epochs, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1 \t Batches:   500/66500 \t                 Time: 241.55s \t Accuracy:    0.521\n"
     ]
    }
   ],
   "source": [
    "# The Training Process\n",
    "total_accuracy = None\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    epoch_start_time = time.time()\n",
    "    # Train one full epoch\n",
    "    train_one_epoch(classifier_model, train_dataloader, optimizer, criterion, gradient_clip, epoch)\n",
    "\n",
    "    # Evaluate\n",
    "    val_accuracy = evaluate(classifier_model, val_dataloader)\n",
    "    if total_accuracy is not None and total_accuracy > val_accuracy:\n",
    "        # update the learning rate\n",
    "        lr_scheduler.step()\n",
    "    else:\n",
    "        # Consider the current accuracy as best accuracy\n",
    "        total_accuracy = val_accuracy\n",
    "    \n",
    "    # Print the metrics at the end of every epoch\n",
    "    print('-' * 60)\n",
    "    time_taken = time.time() - epoch_start_time\n",
    "    print(f\"End of epoch:{epoch:3d} \\t Time taken:{time_taken:5.2f}s \\t Val Accuracy:{val_accuracy:8.3f}\")\n",
    "    print('-' * 60)\n",
    "\n",
    "print(\"End of training process\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model Performance on the Test data\")\n",
    "test_accuracy = evaluate(classifier_model, test_dataloader)\n",
    "print(f\"Test Accuracy:{test_accuracy:8.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "224e3b60dcce273f5f7665f5803a94c2258e151dae11dd21b290a6633eac9c6a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('LAB_VENV': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
